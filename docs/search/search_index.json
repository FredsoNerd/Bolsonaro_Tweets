{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bolsonaro Tweets Projeto final da disciplina de fundamentos de ci\u00eancia dos dados ministrada pelo professor Jorge Poco. Em uma era de alta conectividade e um crescimento da polariza\u00e7\u00e3o pol\u00edtica em diversos pa\u00edses da Am\u00e9rica, a grande quantidade de informa\u00e7\u00e3o disseminada \u00e9 capaz de transformar a sociedade, principalmente quando \u00e9 apresentada por autoridades do pa\u00eds. Por esse motivo, o projeto tem como objetivo analisar o impacto socioecon\u00f4mico dos tweets do presidente da Rep\u00fablica Jair Bolsonaro e seus familiares tem no Brasil, a an\u00e1lise \u00e9 feita atrav\u00e9s de um estudo de diferentes indicadores econ\u00f4micos e do acervo de tweets do presidente. Desenvolvimento Data scrapping Os dados foram obtidos atrav\u00e9s da API distribu\u00edda pelo Twitter e com o aux\u00edlio da biblioteca Tweepy. De acordo com as limita\u00e7\u00f5es de coleta de dados, foram coletados os \u00faltimos 3200 tweets das contas @jairbolsonaro (Jair Bolsonaro), @bolsonarosp (Eduardo Bolsonaro), @carlosbolsonaro (Carlos Bolsonaro) e @flaviobolsonaro (Fl\u00e1vio Bolsonaro), entre os dados apresentados pela ferramenta, foram selecionados para compor o conjunto de dados: nome do usu\u00e1rio, data do tweet, texto do tweet, tipo de m\u00eddia do tweet, lista de hashtags, lista de men\u00e7\u00f5es a usu\u00e1rios, quantidade de retweets, quantidade de favoritos. Os dados de indicadores econ\u00f4micos foram obtidos atrav\u00e9s da API do Banco Central do Brasil que disponibiliza diversas s\u00e9ries temporais, as escolhidas para compor a base de dados do projeto s\u00e3o: IPCA, INPC, IGP-M, meta SELIC, CDI, PIB, d\u00f3lar, d\u00edvida p\u00fablica, reserva internacional, \u00edndice de emprego formal, PNADC, \u00edndice de confian\u00e7a do consumidor.","title":"Home"},{"location":"#bolsonaro-tweets","text":"Projeto final da disciplina de fundamentos de ci\u00eancia dos dados ministrada pelo professor Jorge Poco. Em uma era de alta conectividade e um crescimento da polariza\u00e7\u00e3o pol\u00edtica em diversos pa\u00edses da Am\u00e9rica, a grande quantidade de informa\u00e7\u00e3o disseminada \u00e9 capaz de transformar a sociedade, principalmente quando \u00e9 apresentada por autoridades do pa\u00eds. Por esse motivo, o projeto tem como objetivo analisar o impacto socioecon\u00f4mico dos tweets do presidente da Rep\u00fablica Jair Bolsonaro e seus familiares tem no Brasil, a an\u00e1lise \u00e9 feita atrav\u00e9s de um estudo de diferentes indicadores econ\u00f4micos e do acervo de tweets do presidente.","title":"Bolsonaro Tweets"},{"location":"#desenvolvimento","text":"","title":"Desenvolvimento"},{"location":"#data-scrapping","text":"Os dados foram obtidos atrav\u00e9s da API distribu\u00edda pelo Twitter e com o aux\u00edlio da biblioteca Tweepy. De acordo com as limita\u00e7\u00f5es de coleta de dados, foram coletados os \u00faltimos 3200 tweets das contas @jairbolsonaro (Jair Bolsonaro), @bolsonarosp (Eduardo Bolsonaro), @carlosbolsonaro (Carlos Bolsonaro) e @flaviobolsonaro (Fl\u00e1vio Bolsonaro), entre os dados apresentados pela ferramenta, foram selecionados para compor o conjunto de dados: nome do usu\u00e1rio, data do tweet, texto do tweet, tipo de m\u00eddia do tweet, lista de hashtags, lista de men\u00e7\u00f5es a usu\u00e1rios, quantidade de retweets, quantidade de favoritos. Os dados de indicadores econ\u00f4micos foram obtidos atrav\u00e9s da API do Banco Central do Brasil que disponibiliza diversas s\u00e9ries temporais, as escolhidas para compor a base de dados do projeto s\u00e3o: IPCA, INPC, IGP-M, meta SELIC, CDI, PIB, d\u00f3lar, d\u00edvida p\u00fablica, reserva internacional, \u00edndice de emprego formal, PNADC, \u00edndice de confian\u00e7a do consumidor.","title":"Data scrapping"},{"location":"EconomicData_and_Twitter_EDA/","text":"An\u00e1lise explorat\u00f3ria dos tweets e dos dados econ\u00f4micos import pandas as pd from matplotlib.ticker import PercentFormatter import matplotlib.pyplot as plt import seaborn as sns import numpy as np from sklearn.preprocessing import StandardScaler %matplotlib inline Dados econ\u00f4micos Validez dos dados ## economic_data = pd.read_csv(\"..\\\\data\\\\economic_time_series.csv\", sep=\";\") ## economic_data = pd.read_csv(\"data/economic_time_series.csv\", sep=\";\", index_col=0) economic_data = pd.read_csv(\"../../data/economic_data/economic_time_series.csv\", sep=\";\", index_col=0) # Useful parameters for the code initialDate = '2010-01-01' indices = list(economic_data.columns) nColumn = 3 nRows = int(len(indices)/nColumn) # Y Label, Graph title dicTitles = {'ipca':['Monthly Percentage Value','IPCA'], 'igpm':['Monthly Percentage Value','IGP-M'], 'inpc':['Monthly Percentage Value','INPC'], 'selic_meta':['Yearly Percentage Value','SELIC Meta'], 'international_reserve':['Total daily value (millions of USD)','International Reserve'], 'pnad':['Percentage Value','PNAD'], 'cdi':['Daily Percentage Value','CDI'], 'gdp':['Monthly Values (millions of R$)','GDP'], 'dollar':['Brazilian Reais','Dollar Exchange'], 'employment':['Index Value','Formal Employment'], 'gov_debt':['Percentage Value in relation \\n to GDP (Liquid Debt of Public Sector)','Governamental Debt'], 'consumer_confidence':['Index Value','Consumer Confidence'] } # Plot fig, ax = plt.subplots(nRows, nColumn, figsize=(8*nColumn,6*nRows)) for i in range(0,len(indices)): # Getting the info allValues = np.array(economic_data[indices[i]]) ind = ~np.isnan(allValues) & (np.array(economic_data.index)>=initialDate) seriesValues = allValues[ind] dateValues = pd.to_datetime(economic_data.index[ind]) # Plotting and formatting ax[int(i/nColumn)][i%nColumn].plot(dateValues, seriesValues, lw=0.8) # Set axes limits ax[int(i/nColumn)][i%nColumn].set_xlim(dateValues.min(), dateValues.max()) # Make labels ax[int(i/nColumn)][i%nColumn].set_xlabel('Year', fontsize=14) if 'Percentage Value' in dicTitles[indices[i]][0]: ax[int(i/nColumn)][i%nColumn].set_ylabel(dicTitles[indices[i]][0], fontsize=14) ax[int(i/nColumn)][i%nColumn].yaxis.set_major_formatter(PercentFormatter(100)) else: ax[int(i/nColumn)][i%nColumn].set_ylabel(dicTitles[indices[i]][0], fontsize=14) ax[int(i/nColumn)][i%nColumn].set_title(dicTitles[indices[i]][1], fontsize=18) # Grid ax[int(i/nColumn)][i%nColumn].grid(True, lw=1.5, ls='--', alpha=0.75) # Overall title plt.suptitle(\"Variation of diverse economic indexes\",x=0.5, y=0.93,verticalalignment='top', fontsize = 20) plt.savefig('images/figure1.pdf') plt.show() A vari\u00e1vel abaixo foi criada com o intuito de se encaixar em uma visualiza\u00e7\u00e3o, mas n\u00e3o conseguimos encontrar algum jeito interessante de informar os resultados. importantDates = {'2013 FIFA Confederations Cup':['2013-06-15','2013-06-30'], # soccer related '2014 World Cup':['2014-06-12','2014-07-13'], '2016 Summer Olympics ':['2016-08-05','2016-08-21'], '2019 America\\'s cup':['2019-06-14','2019-07-07'], 'Mariana dam disaster':['2015-11-05','2015-11-22'], # disasters related 'Brumadinho dam disaster':['2019-01-25','2019-01-30'], 'Protests against bus fares':['2013-06-11','2013-06-19'], # politicals things 16 de maio de 2014 '2014 Brazilian economic crisis':['2014-05-16','2016-09-01'], 'Impeachment of Dilma Rousseff':['2016-08-24','2016-08-31'], 'Petrobras investigation through Operation Car Wash':['2014-03-01','2017-04-01'], 'New president':['2019-01-01','2019-02-01'], '2019 Northeast Brazil oil spill':['2019-08-30','2019-10-31'], 'Covid Pandemic':['2020-02-03','2020-06-08'] # current state of things } for key in importantDates.keys(): importantDates[key] = [pd.to_datetime(date) for date in importantDates[key]] An\u00e1lise de correla\u00e7\u00e3o ## we remove days before 2010 for our analisys economic_data.index = pd.to_datetime(economic_data.index) economic_data[\"year\"] = pd.DatetimeIndex(economic_data.index).year economic_data = economic_data[economic_data.year >= 2010] # remove the NaN lines for simplicity clean_economic_data = economic_data.dropna() economic_features = clean_economic_data.columns # rescale the data to better visualization scaler = StandardScaler().fit(clean_economic_data) scaled_economic_data = scaler.transform(clean_economic_data) scaled_economic_data = pd.DataFrame(scaled_economic_data, columns=economic_features) ## plot the data to show correlation corr_mat = np.corrcoef(scaled_economic_data.T) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].set_xticks(range(len(scaled_economic_data.columns))) ax[0].set_xticklabels(scaled_economic_data.columns, rotation=\"vertical\") ax[0].pcolor(corr_mat) ax[0].set_title(\"Economic data correlation matrix\", fontsize=15) ax[1].set_xticks(range(len(scaled_economic_data.columns))) ax[1].set_xticklabels(scaled_economic_data.columns, rotation='vertical') ax[1].pcolor(abs(corr_mat) > 0.8) ax[1].set_title(\"Economic data strong correlation\", fontsize=15) plt.savefig('images/figure2.pdf') plt.show() Muitos dados apresentados s\u00e3o fortemente correlacionados, o que se \u00e9 de esperar: flutua\u00e7\u00f5es em fatores econ\u00f4micos globais influenciam de maneira semelhante \u00edndices semelhantes no mercado. Algumas vari\u00e1veis possuem tamb\u00e9m comportamentos opostos, ou negativamente correlacinados . Para fins de primeira an\u00e1lise, removemos as vari\u00e1veis correlacionadas, evitando ac\u00famulo de informa\u00e7\u00e3o repetida. # we remove strongly correlated features to_remove = [] for col_i in range(len(economic_features)): for col_j in range(col_i+1, len(economic_features)): # if a feature is already to remove, go ahead if economic_features[col_j] in to_remove: continue # if a feature is correlated to a preceding, remove it if abs(corr_mat[col_i,col_j]) > 0.8 : to_remove.append(economic_features[col_j]) # print obtained results print(\"From {} indexes, remove {} of them\".format( len(economic_features), len(to_remove))) print(\"Remove: \", to_remove) From 13 indexes, remove 6 of them Remove: ['inpc', 'cdi', 'employment', 'gov_debt', 'year', 'dollar'] # use the simplified version of the data simp_economic_data = economic_data.drop(to_remove, axis=1) # remove the NaN lines for simplicity clean_economic_data = simp_economic_data.dropna() economic_features = clean_economic_data.columns # rescale the data to better visualization scaler = StandardScaler().fit(clean_economic_data) scaled_economic_data = clean_economic_data.copy() scaled_economic_data[economic_features] = scaler.transform(clean_economic_data) scaled_economic_data = pd.DataFrame(scaled_economic_data) scaled_economic_data.columns = economic_features # plotamos os dados representativos g = scaled_economic_data.plot(figsize=(15,5)) g.set_title(\"Scaled Representative Economic Data (2010 - 2020)\") plt.savefig('images/figure3.pdf') Resumimos o conjunto de vari\u00e1veis inicial a 7 representativas, a partir das quais podemos analisar as tendencias do mercado, e considerar impactos de eventos econ\u00f4micos relevantes em certos per\u00edodos como a crise de 2014 e a elei\u00e7\u00e3o de Donald Trump em 2016. An\u00e1lise de texto do Twitter Desejamos fazer uma primeira an\u00e1lise do conte\u00fados dos Tweets da fam\u00edlia Bolsonaro, al\u00e9m da an\u00e1lise quantitativa. # tweeys_data = pd.read_csv(\"..\\\\data\\\\preprocessed_tweets.csv\", sep=\"~\") # tweeys_data = pd.read_csv(\"data/preprocessed_tweets.csv\", sep=\"~\", index_col=0) tweeys_data = pd.read_csv(\"../../data/tweets/preprocessed_tweets.csv\", sep=\"~\", index_col=0) tweeys_data[\"date\"] = pd.to_datetime(tweeys_data[\"date\"]) tweeys_data.set_index(\"date\", inplace=True) fig, ax = plt.subplots(figsize=(15,5)) sns.scatterplot(x = tweeys_data.index, y = \"retweet_count\", data=tweeys_data, hue=\"name\", alpha=0.5, ax=ax) ax.set_xlabel(\"Date (2017-2020)\", fontsize=15) ax.set_ylabel(\"#retweets\", fontsize=15) ax.set_title(\"Retweets quantity (represented influence)\", fontsize=20) ax.set_xlim('2017-06-10', '2020-08-07') plt.savefig('images/figure4.pdf') plt.show() Discuss\u00e3o de s\u00e9ries temporais: tweets e dados econ\u00f4micos Levantamos alguns questionamentos das s\u00e9ries com rela\u00e7\u00e3o alguns eventos econ\u00f4micos que podem representar impactos relevantes, assim como a rela\u00e7\u00e3o de influencia/correla\u00e7\u00e3o dos tweets com o mercado. Quais eventos entre 2010 e 2020 explicam impactos, ou comportamentos externos nos dados? Esses impactos devem ser removidos, ou o mais adequado \u00e9 mant\u00ea-los em an\u00e1lise como explicadores de eventuais correla\u00e7\u00f5es com os tweets? Como podemos relacionar termos mais frequentes e um per\u00edodo com os fatos econ\u00f4micos apresentados? Quando termos s\u00e3o anteriores \u00e0s not\u00edcias deve\u00edculos importantes no mesmo per\u00edodo, isso prediz impacto dos tweets na economia? Resumo de algumas not\u00edcias na \u00fatimas d\u00e9cada: https://pt.wikipedia.org/wiki/D%C3%A9cada_de_2010#Cronologia_de_eventos monthly_tweeys_data = tweeys_data.groupby(['year','month','name']).agg({ 'name': len }) monthly_tweeys_data.columns = ['numberOfTweets'] economic_data['month'] =economic_data.index.month monthly_economic_data = economic_data.groupby(['year','month']).agg({ 'ipca': np.mean, 'igpm': np.mean, 'inpc': np.mean, 'selic_meta': np.mean, 'international_reserve': np.mean, 'pnad': np.mean, 'cdi': np.mean, 'gdp': np.mean, 'dollar': np.mean, 'employment': np.mean, 'gov_debt': np.mean, 'consumer_confidence': np.mean }) monthly_tweeys_data = monthly_tweeys_data.reset_index() monthly_economic_data = monthly_economic_data.reset_index() monthly_tweeys_data['date'] = [pd.datetime(year=monthly_tweeys_data.iloc[i,0], month=monthly_tweeys_data.iloc[i,1], day=1) for i in range(0,len(monthly_tweeys_data))] monthly_economic_data['date'] = [pd.datetime(year=monthly_economic_data.iloc[i,0], month=monthly_economic_data.iloc[i,1], day=1) for i in range(0,len(monthly_economic_data))] names = monthly_tweeys_data['name'].unique() economyIndexesPercentageBased = ['ipca','igpm','pnad','selic_meta'] # Plot fig, ax = plt.subplots(2, 1, figsize=(10,12)) # Getting the info and plotting for name in names: ax[0].plot(monthly_tweeys_data['date'].loc[monthly_tweeys_data['name']==name], monthly_tweeys_data['numberOfTweets'].loc[monthly_tweeys_data['name']==name], label=name) # Aesthetics stuff ax[0].set_xlim('2019-05-01','2020-07-01') ax[0].set_xlabel('Date') ax[0].set_ylabel('Number of tweets') ax[0].set_title('Bolsonaro family monthly tweets') ax[0].grid(True, lw=1.5, ls='--', alpha=0.75) ax[0].legend() # Getting the info and plotting for ecoIndex in economyIndexesPercentageBased: ind = ~np.isnan(monthly_economic_data[ecoIndex]) ax[1].plot(monthly_economic_data['date'][ind], monthly_economic_data[ecoIndex][ind], label=ecoIndex) # Aesthetics stuff ax[1].set_xlim('2019-05-01','2020-07-01') ax[1].set_xlabel('Date') ax[1].set_ylabel('Index percentage value') ax[1].set_title('Diverse economy indexes') ax[1].grid(True, lw=1.5, ls='--', alpha=0.75) ax[1].yaxis.set_major_formatter(PercentFormatter(100)) ax[1].legend() # Overall title plt.suptitle(\"Influence of the amount of tweets on economy\",x=0.5, y=0.95,verticalalignment='top', fontsize = 20) plt.savefig('images/figure5.pdf') plt.show() Nuvem de palavras Desejamos analisar quais os principais assuntos presentes no tweets do perfil do Jair Bolsonaro e tamb\u00e9m analisar como esses t\u00f3picos se alteraram ao longo do tempo, para isso, produziremos nuvens de palavras. Para realizar a visualiza\u00e7\u00e3o de nuvem de palavras utilizaremos a biblioteca wordcloud e a partir dos dados dos tweets do Bolsonaro executaremos uma limpeza para adequar os dados para a biblioteca, para isso \u00e9 necess\u00e1rio a biblioteca nltk com seu conjunto de stopwords para que n\u00e3o sejam consideradas na visualiza\u00e7\u00e3o. from wordcloud import WordCloud, STOPWORDS from PIL import Image import nltk nltk.download('stopwords') stopwords = nltk.corpus.stopwords.words('portuguese') [nltk_data] Downloading package stopwords to [nltk_data] C:\\Users\\giova\\AppData\\Roaming\\nltk_data... [nltk_data] Package stopwords is already up-to-date! tweet_df = pd.read_csv(\"preprocessed_tweets.csv\", sep = \"~\").drop(columns = [\"Unnamed: 0\"]) bolsonaro_df = tweet_df[tweet_df.name == \"jairbolsonaro\"] print(\"O tweet mais antigo \u00e9 de \", bolsonaro_df.date.min()) print(\"O tweet mais recente \u00e9 de \", bolsonaro_df.date.max()) O tweet mais antigo \u00e9 de 2019-05-16 17:22:36+00:00 O tweet mais recente \u00e9 de 2020-07-27 20:51:13+00:00 plot_data = bolsonaro_df.groupby(by = [\"year\", \"month\"]).count().reset_index() plot_data[\"year-monthly\"] = plot_data.month.apply(str) + \"/\" + plot_data.year.apply(str) plt.plot(plot_data['year-monthly'], plot_data.full_text) plt.title(\"N\u00famero de tweets do Bolsonaro por m\u00eas\") plt.xticks(rotation = 90) plt.grid() plt.ylim((0, 370)) plt.show() Observando os dados que possu\u00edmos, dividiremos em 4 intervalos temporais para produzir 4 nuvens de palavras de cada um desses intervalos. Os intervalos ser\u00e3o: In\u00edcio do dados at\u00e9 01/09/2019. 01/09/2019 at\u00e9 31/12/2019 01/01/2020 at\u00e9 01/04/2020 01/04/2020 at\u00e9 o final dos dados. def convert_tweets_to_text(df): df = df.full_text.str.lower() #everything lowercase df = df.str.replace('^https?:\\/\\/.*[\\r\\n]*', '', regex = True) #remove urls df = df.str.replace('[^A-z\u00c0-\u00fa0-9 ]', '', regex = True) #keeps only alphanumerics and space return \"\".join(df.tolist()) tweets_start_sep = convert_tweets_to_text(bolsonaro_df[((bolsonaro_df.date.min() < bolsonaro_df.date) & (bolsonaro_df.date < \"2019-09-01\"))]) tweets_sep_dec = convert_tweets_to_text(bolsonaro_df[((\"2019-09-01\" <= bolsonaro_df.date) & (bolsonaro_df.date <= \"2019-12-31\"))]) tweets_jan_mar = convert_tweets_to_text(bolsonaro_df[((\"2020-01-01\" <= bolsonaro_df.date) & (bolsonaro_df.date < \"2020-04-01\"))]) tweets_apr_end = convert_tweets_to_text(bolsonaro_df[((\"2020-04-01\" <= bolsonaro_df.date) & (bolsonaro_df.date <= bolsonaro_df.date.max()))]) wordcloud1 = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(tweets_start_sep) wordcloud2 = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(tweets_sep_dec) wordcloud3 = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(tweets_jan_mar) wordcloud4 = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(tweets_apr_end) # plot the WordCloud images fig = plt.figure(figsize = (14, 14), facecolor = None) #fig.tight_layout(rect=[0, 0.05, 1, 0.95]) fig.suptitle(\"Nuvem de palavras utilizadas no Twitter por Bolsonaro\", fontsize=24, y = 0.95) ax1 = plt.subplot(221) plt.imshow(wordcloud1) plt.axis(\"off\") ax1.set_title(\"Junho - Setembro 2019\", fontsize=16) ax2 = plt.subplot(222) plt.imshow(wordcloud2) plt.axis(\"off\") ax2.set_title(\"Setembro - Dezembro 2019\", fontsize=16) ax3 = plt.subplot(223) plt.imshow(wordcloud3) plt.axis(\"off\") ax3.set_title(\"Janeiro - Mar\u00e7o 2020\", fontsize=16) ax4 = plt.subplot(224) plt.imshow(wordcloud4) plt.axis(\"off\") ax4.set_title(\"Mar\u00e7o - Julho 2020\", fontsize=16) plt.tight_layout(pad = 0.05) plt.subplots_adjust(top=0.87) plt.savefig('..\\\\images\\\\wordcloud.pdf', pad_inches = 1) plt.show()","title":"An\u00e1lise explorat\u00f3ria de dados"},{"location":"EconomicData_and_Twitter_EDA/#analise-exploratoria-dos-tweets-e-dos-dados-economicos","text":"import pandas as pd from matplotlib.ticker import PercentFormatter import matplotlib.pyplot as plt import seaborn as sns import numpy as np from sklearn.preprocessing import StandardScaler %matplotlib inline","title":"An\u00e1lise explorat\u00f3ria dos tweets e dos dados econ\u00f4micos"},{"location":"EconomicData_and_Twitter_EDA/#dados-economicos","text":"","title":"Dados econ\u00f4micos"},{"location":"EconomicData_and_Twitter_EDA/#validez-dos-dados","text":"## economic_data = pd.read_csv(\"..\\\\data\\\\economic_time_series.csv\", sep=\";\") ## economic_data = pd.read_csv(\"data/economic_time_series.csv\", sep=\";\", index_col=0) economic_data = pd.read_csv(\"../../data/economic_data/economic_time_series.csv\", sep=\";\", index_col=0) # Useful parameters for the code initialDate = '2010-01-01' indices = list(economic_data.columns) nColumn = 3 nRows = int(len(indices)/nColumn) # Y Label, Graph title dicTitles = {'ipca':['Monthly Percentage Value','IPCA'], 'igpm':['Monthly Percentage Value','IGP-M'], 'inpc':['Monthly Percentage Value','INPC'], 'selic_meta':['Yearly Percentage Value','SELIC Meta'], 'international_reserve':['Total daily value (millions of USD)','International Reserve'], 'pnad':['Percentage Value','PNAD'], 'cdi':['Daily Percentage Value','CDI'], 'gdp':['Monthly Values (millions of R$)','GDP'], 'dollar':['Brazilian Reais','Dollar Exchange'], 'employment':['Index Value','Formal Employment'], 'gov_debt':['Percentage Value in relation \\n to GDP (Liquid Debt of Public Sector)','Governamental Debt'], 'consumer_confidence':['Index Value','Consumer Confidence'] } # Plot fig, ax = plt.subplots(nRows, nColumn, figsize=(8*nColumn,6*nRows)) for i in range(0,len(indices)): # Getting the info allValues = np.array(economic_data[indices[i]]) ind = ~np.isnan(allValues) & (np.array(economic_data.index)>=initialDate) seriesValues = allValues[ind] dateValues = pd.to_datetime(economic_data.index[ind]) # Plotting and formatting ax[int(i/nColumn)][i%nColumn].plot(dateValues, seriesValues, lw=0.8) # Set axes limits ax[int(i/nColumn)][i%nColumn].set_xlim(dateValues.min(), dateValues.max()) # Make labels ax[int(i/nColumn)][i%nColumn].set_xlabel('Year', fontsize=14) if 'Percentage Value' in dicTitles[indices[i]][0]: ax[int(i/nColumn)][i%nColumn].set_ylabel(dicTitles[indices[i]][0], fontsize=14) ax[int(i/nColumn)][i%nColumn].yaxis.set_major_formatter(PercentFormatter(100)) else: ax[int(i/nColumn)][i%nColumn].set_ylabel(dicTitles[indices[i]][0], fontsize=14) ax[int(i/nColumn)][i%nColumn].set_title(dicTitles[indices[i]][1], fontsize=18) # Grid ax[int(i/nColumn)][i%nColumn].grid(True, lw=1.5, ls='--', alpha=0.75) # Overall title plt.suptitle(\"Variation of diverse economic indexes\",x=0.5, y=0.93,verticalalignment='top', fontsize = 20) plt.savefig('images/figure1.pdf') plt.show() A vari\u00e1vel abaixo foi criada com o intuito de se encaixar em uma visualiza\u00e7\u00e3o, mas n\u00e3o conseguimos encontrar algum jeito interessante de informar os resultados. importantDates = {'2013 FIFA Confederations Cup':['2013-06-15','2013-06-30'], # soccer related '2014 World Cup':['2014-06-12','2014-07-13'], '2016 Summer Olympics ':['2016-08-05','2016-08-21'], '2019 America\\'s cup':['2019-06-14','2019-07-07'], 'Mariana dam disaster':['2015-11-05','2015-11-22'], # disasters related 'Brumadinho dam disaster':['2019-01-25','2019-01-30'], 'Protests against bus fares':['2013-06-11','2013-06-19'], # politicals things 16 de maio de 2014 '2014 Brazilian economic crisis':['2014-05-16','2016-09-01'], 'Impeachment of Dilma Rousseff':['2016-08-24','2016-08-31'], 'Petrobras investigation through Operation Car Wash':['2014-03-01','2017-04-01'], 'New president':['2019-01-01','2019-02-01'], '2019 Northeast Brazil oil spill':['2019-08-30','2019-10-31'], 'Covid Pandemic':['2020-02-03','2020-06-08'] # current state of things } for key in importantDates.keys(): importantDates[key] = [pd.to_datetime(date) for date in importantDates[key]]","title":"Validez dos dados"},{"location":"EconomicData_and_Twitter_EDA/#analise-de-correlacao","text":"## we remove days before 2010 for our analisys economic_data.index = pd.to_datetime(economic_data.index) economic_data[\"year\"] = pd.DatetimeIndex(economic_data.index).year economic_data = economic_data[economic_data.year >= 2010] # remove the NaN lines for simplicity clean_economic_data = economic_data.dropna() economic_features = clean_economic_data.columns # rescale the data to better visualization scaler = StandardScaler().fit(clean_economic_data) scaled_economic_data = scaler.transform(clean_economic_data) scaled_economic_data = pd.DataFrame(scaled_economic_data, columns=economic_features) ## plot the data to show correlation corr_mat = np.corrcoef(scaled_economic_data.T) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].set_xticks(range(len(scaled_economic_data.columns))) ax[0].set_xticklabels(scaled_economic_data.columns, rotation=\"vertical\") ax[0].pcolor(corr_mat) ax[0].set_title(\"Economic data correlation matrix\", fontsize=15) ax[1].set_xticks(range(len(scaled_economic_data.columns))) ax[1].set_xticklabels(scaled_economic_data.columns, rotation='vertical') ax[1].pcolor(abs(corr_mat) > 0.8) ax[1].set_title(\"Economic data strong correlation\", fontsize=15) plt.savefig('images/figure2.pdf') plt.show() Muitos dados apresentados s\u00e3o fortemente correlacionados, o que se \u00e9 de esperar: flutua\u00e7\u00f5es em fatores econ\u00f4micos globais influenciam de maneira semelhante \u00edndices semelhantes no mercado. Algumas vari\u00e1veis possuem tamb\u00e9m comportamentos opostos, ou negativamente correlacinados . Para fins de primeira an\u00e1lise, removemos as vari\u00e1veis correlacionadas, evitando ac\u00famulo de informa\u00e7\u00e3o repetida. # we remove strongly correlated features to_remove = [] for col_i in range(len(economic_features)): for col_j in range(col_i+1, len(economic_features)): # if a feature is already to remove, go ahead if economic_features[col_j] in to_remove: continue # if a feature is correlated to a preceding, remove it if abs(corr_mat[col_i,col_j]) > 0.8 : to_remove.append(economic_features[col_j]) # print obtained results print(\"From {} indexes, remove {} of them\".format( len(economic_features), len(to_remove))) print(\"Remove: \", to_remove) From 13 indexes, remove 6 of them Remove: ['inpc', 'cdi', 'employment', 'gov_debt', 'year', 'dollar'] # use the simplified version of the data simp_economic_data = economic_data.drop(to_remove, axis=1) # remove the NaN lines for simplicity clean_economic_data = simp_economic_data.dropna() economic_features = clean_economic_data.columns # rescale the data to better visualization scaler = StandardScaler().fit(clean_economic_data) scaled_economic_data = clean_economic_data.copy() scaled_economic_data[economic_features] = scaler.transform(clean_economic_data) scaled_economic_data = pd.DataFrame(scaled_economic_data) scaled_economic_data.columns = economic_features # plotamos os dados representativos g = scaled_economic_data.plot(figsize=(15,5)) g.set_title(\"Scaled Representative Economic Data (2010 - 2020)\") plt.savefig('images/figure3.pdf') Resumimos o conjunto de vari\u00e1veis inicial a 7 representativas, a partir das quais podemos analisar as tendencias do mercado, e considerar impactos de eventos econ\u00f4micos relevantes em certos per\u00edodos como a crise de 2014 e a elei\u00e7\u00e3o de Donald Trump em 2016.","title":"An\u00e1lise de correla\u00e7\u00e3o"},{"location":"EconomicData_and_Twitter_EDA/#analise-de-texto-do-twitter","text":"Desejamos fazer uma primeira an\u00e1lise do conte\u00fados dos Tweets da fam\u00edlia Bolsonaro, al\u00e9m da an\u00e1lise quantitativa. # tweeys_data = pd.read_csv(\"..\\\\data\\\\preprocessed_tweets.csv\", sep=\"~\") # tweeys_data = pd.read_csv(\"data/preprocessed_tweets.csv\", sep=\"~\", index_col=0) tweeys_data = pd.read_csv(\"../../data/tweets/preprocessed_tweets.csv\", sep=\"~\", index_col=0) tweeys_data[\"date\"] = pd.to_datetime(tweeys_data[\"date\"]) tweeys_data.set_index(\"date\", inplace=True) fig, ax = plt.subplots(figsize=(15,5)) sns.scatterplot(x = tweeys_data.index, y = \"retweet_count\", data=tweeys_data, hue=\"name\", alpha=0.5, ax=ax) ax.set_xlabel(\"Date (2017-2020)\", fontsize=15) ax.set_ylabel(\"#retweets\", fontsize=15) ax.set_title(\"Retweets quantity (represented influence)\", fontsize=20) ax.set_xlim('2017-06-10', '2020-08-07') plt.savefig('images/figure4.pdf') plt.show()","title":"An\u00e1lise de texto do Twitter"},{"location":"EconomicData_and_Twitter_EDA/#discussao-de-series-temporais-tweets-e-dados-economicos","text":"Levantamos alguns questionamentos das s\u00e9ries com rela\u00e7\u00e3o alguns eventos econ\u00f4micos que podem representar impactos relevantes, assim como a rela\u00e7\u00e3o de influencia/correla\u00e7\u00e3o dos tweets com o mercado. Quais eventos entre 2010 e 2020 explicam impactos, ou comportamentos externos nos dados? Esses impactos devem ser removidos, ou o mais adequado \u00e9 mant\u00ea-los em an\u00e1lise como explicadores de eventuais correla\u00e7\u00f5es com os tweets? Como podemos relacionar termos mais frequentes e um per\u00edodo com os fatos econ\u00f4micos apresentados? Quando termos s\u00e3o anteriores \u00e0s not\u00edcias deve\u00edculos importantes no mesmo per\u00edodo, isso prediz impacto dos tweets na economia? Resumo de algumas not\u00edcias na \u00fatimas d\u00e9cada: https://pt.wikipedia.org/wiki/D%C3%A9cada_de_2010#Cronologia_de_eventos monthly_tweeys_data = tweeys_data.groupby(['year','month','name']).agg({ 'name': len }) monthly_tweeys_data.columns = ['numberOfTweets'] economic_data['month'] =economic_data.index.month monthly_economic_data = economic_data.groupby(['year','month']).agg({ 'ipca': np.mean, 'igpm': np.mean, 'inpc': np.mean, 'selic_meta': np.mean, 'international_reserve': np.mean, 'pnad': np.mean, 'cdi': np.mean, 'gdp': np.mean, 'dollar': np.mean, 'employment': np.mean, 'gov_debt': np.mean, 'consumer_confidence': np.mean }) monthly_tweeys_data = monthly_tweeys_data.reset_index() monthly_economic_data = monthly_economic_data.reset_index() monthly_tweeys_data['date'] = [pd.datetime(year=monthly_tweeys_data.iloc[i,0], month=monthly_tweeys_data.iloc[i,1], day=1) for i in range(0,len(monthly_tweeys_data))] monthly_economic_data['date'] = [pd.datetime(year=monthly_economic_data.iloc[i,0], month=monthly_economic_data.iloc[i,1], day=1) for i in range(0,len(monthly_economic_data))] names = monthly_tweeys_data['name'].unique() economyIndexesPercentageBased = ['ipca','igpm','pnad','selic_meta'] # Plot fig, ax = plt.subplots(2, 1, figsize=(10,12)) # Getting the info and plotting for name in names: ax[0].plot(monthly_tweeys_data['date'].loc[monthly_tweeys_data['name']==name], monthly_tweeys_data['numberOfTweets'].loc[monthly_tweeys_data['name']==name], label=name) # Aesthetics stuff ax[0].set_xlim('2019-05-01','2020-07-01') ax[0].set_xlabel('Date') ax[0].set_ylabel('Number of tweets') ax[0].set_title('Bolsonaro family monthly tweets') ax[0].grid(True, lw=1.5, ls='--', alpha=0.75) ax[0].legend() # Getting the info and plotting for ecoIndex in economyIndexesPercentageBased: ind = ~np.isnan(monthly_economic_data[ecoIndex]) ax[1].plot(monthly_economic_data['date'][ind], monthly_economic_data[ecoIndex][ind], label=ecoIndex) # Aesthetics stuff ax[1].set_xlim('2019-05-01','2020-07-01') ax[1].set_xlabel('Date') ax[1].set_ylabel('Index percentage value') ax[1].set_title('Diverse economy indexes') ax[1].grid(True, lw=1.5, ls='--', alpha=0.75) ax[1].yaxis.set_major_formatter(PercentFormatter(100)) ax[1].legend() # Overall title plt.suptitle(\"Influence of the amount of tweets on economy\",x=0.5, y=0.95,verticalalignment='top', fontsize = 20) plt.savefig('images/figure5.pdf') plt.show()","title":"Discuss\u00e3o de s\u00e9ries temporais: tweets e dados econ\u00f4micos"},{"location":"EconomicData_and_Twitter_EDA/#nuvem-de-palavras","text":"Desejamos analisar quais os principais assuntos presentes no tweets do perfil do Jair Bolsonaro e tamb\u00e9m analisar como esses t\u00f3picos se alteraram ao longo do tempo, para isso, produziremos nuvens de palavras. Para realizar a visualiza\u00e7\u00e3o de nuvem de palavras utilizaremos a biblioteca wordcloud e a partir dos dados dos tweets do Bolsonaro executaremos uma limpeza para adequar os dados para a biblioteca, para isso \u00e9 necess\u00e1rio a biblioteca nltk com seu conjunto de stopwords para que n\u00e3o sejam consideradas na visualiza\u00e7\u00e3o. from wordcloud import WordCloud, STOPWORDS from PIL import Image import nltk nltk.download('stopwords') stopwords = nltk.corpus.stopwords.words('portuguese') [nltk_data] Downloading package stopwords to [nltk_data] C:\\Users\\giova\\AppData\\Roaming\\nltk_data... [nltk_data] Package stopwords is already up-to-date! tweet_df = pd.read_csv(\"preprocessed_tweets.csv\", sep = \"~\").drop(columns = [\"Unnamed: 0\"]) bolsonaro_df = tweet_df[tweet_df.name == \"jairbolsonaro\"] print(\"O tweet mais antigo \u00e9 de \", bolsonaro_df.date.min()) print(\"O tweet mais recente \u00e9 de \", bolsonaro_df.date.max()) O tweet mais antigo \u00e9 de 2019-05-16 17:22:36+00:00 O tweet mais recente \u00e9 de 2020-07-27 20:51:13+00:00 plot_data = bolsonaro_df.groupby(by = [\"year\", \"month\"]).count().reset_index() plot_data[\"year-monthly\"] = plot_data.month.apply(str) + \"/\" + plot_data.year.apply(str) plt.plot(plot_data['year-monthly'], plot_data.full_text) plt.title(\"N\u00famero de tweets do Bolsonaro por m\u00eas\") plt.xticks(rotation = 90) plt.grid() plt.ylim((0, 370)) plt.show() Observando os dados que possu\u00edmos, dividiremos em 4 intervalos temporais para produzir 4 nuvens de palavras de cada um desses intervalos. Os intervalos ser\u00e3o: In\u00edcio do dados at\u00e9 01/09/2019. 01/09/2019 at\u00e9 31/12/2019 01/01/2020 at\u00e9 01/04/2020 01/04/2020 at\u00e9 o final dos dados. def convert_tweets_to_text(df): df = df.full_text.str.lower() #everything lowercase df = df.str.replace('^https?:\\/\\/.*[\\r\\n]*', '', regex = True) #remove urls df = df.str.replace('[^A-z\u00c0-\u00fa0-9 ]', '', regex = True) #keeps only alphanumerics and space return \"\".join(df.tolist()) tweets_start_sep = convert_tweets_to_text(bolsonaro_df[((bolsonaro_df.date.min() < bolsonaro_df.date) & (bolsonaro_df.date < \"2019-09-01\"))]) tweets_sep_dec = convert_tweets_to_text(bolsonaro_df[((\"2019-09-01\" <= bolsonaro_df.date) & (bolsonaro_df.date <= \"2019-12-31\"))]) tweets_jan_mar = convert_tweets_to_text(bolsonaro_df[((\"2020-01-01\" <= bolsonaro_df.date) & (bolsonaro_df.date < \"2020-04-01\"))]) tweets_apr_end = convert_tweets_to_text(bolsonaro_df[((\"2020-04-01\" <= bolsonaro_df.date) & (bolsonaro_df.date <= bolsonaro_df.date.max()))]) wordcloud1 = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(tweets_start_sep) wordcloud2 = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(tweets_sep_dec) wordcloud3 = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(tweets_jan_mar) wordcloud4 = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(tweets_apr_end) # plot the WordCloud images fig = plt.figure(figsize = (14, 14), facecolor = None) #fig.tight_layout(rect=[0, 0.05, 1, 0.95]) fig.suptitle(\"Nuvem de palavras utilizadas no Twitter por Bolsonaro\", fontsize=24, y = 0.95) ax1 = plt.subplot(221) plt.imshow(wordcloud1) plt.axis(\"off\") ax1.set_title(\"Junho - Setembro 2019\", fontsize=16) ax2 = plt.subplot(222) plt.imshow(wordcloud2) plt.axis(\"off\") ax2.set_title(\"Setembro - Dezembro 2019\", fontsize=16) ax3 = plt.subplot(223) plt.imshow(wordcloud3) plt.axis(\"off\") ax3.set_title(\"Janeiro - Mar\u00e7o 2020\", fontsize=16) ax4 = plt.subplot(224) plt.imshow(wordcloud4) plt.axis(\"off\") ax4.set_title(\"Mar\u00e7o - Julho 2020\", fontsize=16) plt.tight_layout(pad = 0.05) plt.subplots_adjust(top=0.87) plt.savefig('..\\\\images\\\\wordcloud.pdf', pad_inches = 1) plt.show()","title":"Nuvem de palavras"},{"location":"data_scrapping/","text":"Raspagem e prepara\u00e7\u00e3o dos dados Dados do Twitter A etapa inicial do projeto \u00e9 a obten\u00e7\u00e3o dos dados do Twitter, faremos o scraping (raspagem) dos dados, para isso utilizaremos de duas fontes, um conjunto de dados presente no Kraggle e um conjunto de dados obtido atrav\u00e9s da API do Twitter. import pandas as pd import tweepy as tw import pickle as pkl import getpass import time Dados do Kraggle O conjunto de dados do Kraggle Jair Bolsonaro Twitter Data apresenta uma cole\u00e7\u00e3o de 3 mil tweets do Bolsonaro. kraggle_data = pd.read_csv(\"..//data//tweets//bolsonaro_tweets.csv\") kraggle_data.date = pd.to_datetime(kraggle_data.date) kraggle_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date text likes retweets link 0 2020-08-01 - Ministro @tarcisiogdf e os 160 anos da Infra... 880 163 https://twitter.com/jairbolsonaro/status/12895... 1 2020-08-01 - Coletiva com o prefeito de Bag\u00e9/RS, Divaldo ... 336 51 https://twitter.com/jairbolsonaro/status/12895... 2 2020-08-01 @rsallesmma \ud83d\udc4d\ud83c\udffb 442 39 https://twitter.com/jairbolsonaro/status/12893... 3 2020-08-01 @Marceloscf2 @tarcisiogdf Seguimos! \ud83e\udd1d 164 22 https://twitter.com/jairbolsonaro/status/12893... 4 2020-08-01 @ItamaratyGovBr @USAID \ud83d\udc4d\ud83c\udffb 223 24 https://twitter.com/jairbolsonaro/status/12893... print(\"O per\u00edodo de \" + str(len(kraggle_data))+\" tweets \u00e9 de \" + str(kraggle_data.date.min())[0:10] +\" at\u00e9 \" + str(kraggle_data.date.max())[0:10] + \".\") O per\u00edodo de 9351 tweets \u00e9 de 2010-04-01 at\u00e9 2020-08-01. Twitter API N\u00f3s tamb\u00e9m podemos obter dados sobre Jair Bolsonaro, Fl\u00e1vio Bolsonaro, Carlos Bolsonaro e Eduardo Bolsonaro da API do Twitter usando Tweepy. No entanto, o acesso \u00e9 permitido apenas para os \u00faltimos 3200 tweets, dessa forma, n\u00e3o \u00e9 o hist\u00f3rico completo de tweets. Autentica\u00e7\u00e3o consumer_key = getpass.getpass() \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 consumer_secret = getpass.getpass() \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 access_token = getpass.getpass() \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 access_token_secret = getpass.getpass() \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 auth = tw.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_token, access_token_secret) api = tw.API(auth, wait_on_rate_limit=True) #requesting data from twitter twitter_data = {} for user in ['jairbolsonaro', 'bolsonarosp', 'flaviobolsonaro', 'carlosbolsonaro']: twitter_data[user] = [tweet for tweet in tw.Cursor(api.user_timeline,id=user, tweet_mode =\"extended\").items()] #saving original data with open(\"..//data//tweets//brute_scrapping,pkl\", \"wb+\") as f: pkl.dump(twitter_data, f) #script to create a dataframe from the data date = [] text = [] user_name = [] place = [] retweets = [] favorites = [] for user in ['jairbolsonaro', 'bolsonarosp', 'flaviobolsonaro', 'carlosbolsonaro']: for tweet in twitter_data[user]: date.append(tweet._json['created_at']) text.append(tweet._json['full_text']) user_name.append(user) place.append(tweet._json['place']) retweets.append(tweet._json['retweet_count']) favorites.append(tweet._json['favorite_count']) API_data = pd.DataFrame({'name': user_name, 'text': text, 'date': pd.to_datetime(date), 'place': place, 'retweets': retweets, 'favorites':favorites}) API_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name text date place retweets favorites 0 jairbolsonaro -Edif\u00edcio Joelma/SP, 1974.\\n\\n-Sgt CASSANIGA s... 2020-07-27 20:51:13+00:00 None 3154 16202 1 jairbolsonaro - \u00c1gua para quem tem sede.\\n- Liberdade para u... 2020-07-27 11:10:36+00:00 None 8101 37357 2 jairbolsonaro @tarcisiogdf @MInfraestrutura \ud83e\udd1d\ud83c\udde7\ud83c\uddf7, Ministro! 2020-07-26 20:18:19+00:00 None 1074 16840 3 jairbolsonaro 2- @MinEconomia @MinCidadania @onyxlorenzoni @... 2020-07-26 15:40:39+00:00 None 1337 6383 4 jairbolsonaro 1- Acompanhe as redes sociais! @secomvc @fabio... 2020-07-26 15:39:47+00:00 None 3287 14836 API_data.to_csv('..API_data.csv', sep = \"~\") Dados econ\u00f4micos N\u00f3s vamos coletar os dados do BACEN (Banco Central do Brasil) usando sua API. N\u00f3s vamos utilizar as s\u00e9ries temporais de alguns importantes indicadores econ\u00f4micos como GDP, IPCA, NPCC. O c\u00f3digo presente neste notebook \u00e9 baseado no v\u00eddeo de C\u00f3digo Quant - Finan\u00e7as Quantitativas : COMO ACESSAR A BASE DE DADOS DO BANCO CENTRAL DO BRASIL COM PYTHON | Python para Investimentos #10 . def query_bc(code): url = 'http://api.bcb.gov.br/dados/serie/bcdata.sgs.{}/dados?formato=json'.format(code) df = pd.read_json(url) df['data'] = pd.to_datetime(df['data'], dayfirst=True) df.set_index('data', inplace=True) return df As s\u00e9ries temporais escolhidas para compor o nosso conjunto de dados est\u00e3o listadas abaixo, tabm\u00e9m est\u00e1 listado sua unidade e o instituto que publica o indicador: IPCA : \u00cdndice nacional de pre\u00e7os ao consumidor-amplo, Var. % mensal, (IBGE). Este \u00edndice mede a varia\u00e7\u00e3o de pre\u00e7os de mercado para o consumidor final e \u00e9 utilizado para medir a infla\u00e7\u00e3o. INPC: \u00cdndice nacional de pre\u00e7os ao consumidor, Var. % mensal, (IBGE). Este \u00edndice mede a varia\u00e7\u00e3o de pre\u00e7os da cesta de consumo da popula\u00e7\u00e3o assalariada com mais baixo rendimento. IGP-M : \u00cdndice geral de pre\u00e7os do mercado, Var. % mensal, (FGV). Este \u00edndice mede a varia\u00e7\u00e3o de pre\u00e7os atrav\u00e9s de outros \u00edndices, do IPCA, INCC, e IPC. Meta SELIC : Taxa b\u00e1sica de juros definida pelo Copom, % a.a., (Copom). CDI : Taxa de juros, % a.d., (Cetip). Taxa de juros para empr\u00e9stimo entre bancos. PIB : Produto Interno Bruto mensal, R\\$ (milh\u00f5es), (BCB-Depec). D\u00f3lar: Taxa de c\u00e2mbio - D\u00f3lar americano (venda), u.m.c/US\\$, (Sisbacen PTAX800). D\u00edvida p\u00fablica: D\u00edvida l\u00edquida do Setor P\u00fablico (%PIB) - Total, %, (BSB - DSTAT). Reserva Internacional: Reserva internacional total di\u00e1ria, US\\$ (milh\u00f5es), (BCB - DSTAT). \u00cdndice de emprego formal, (Mtb). PNADC: Taxa de desocupa\u00e7\u00e3o, %, (IBGE). \u00cdndice de confian\u00e7a do consumidor, (Fecomercio). ipca = query_bc(433) ipca.to_csv(\"..\\\\data\\\\economic_index\\\\ipca.csv\", sep = \";\") time.sleep(5) igpm = query_bc(189) igpm.to_csv(\"..\\\\data\\\\economic_index\\\\igpm.csv\", sep = \";\") time.sleep(5) inpc = query_bc(188) inpc.to_csv(\"..\\\\data\\\\economic_index\\\\inpc.csv\", sep = \";\") time.sleep(5) selic_meta = query_bc(432) selic_meta.to_csv(\"..\\\\data\\\\economic_index\\\\selic_meta.csv\", sep = \";\") time.sleep(5) international_reserve = query_bc(13621) international_reserve.to_csv(\"..\\\\data\\\\economic_index\\\\international_reserve.csv\", sep = \";\") time.sleep(5) pnad = query_bc(24369) pnad.to_csv(\"..\\\\data\\\\economic_index\\\\pnad.csv\", sep = \";\") time.sleep(5) cdi = query_bc(12) cdi.to_csv(\"..\\\\data\\\\economic_index\\\\cdi.csv\", sep = \";\") time.sleep(5) gdp = query_bc(4385) gdp.to_csv(\"..\\\\data\\\\economic_index\\\\gdp.csv\", sep = \";\") time.sleep(5) dollar = query_bc(1) dollar.to_csv(\"..\\\\data\\\\economic_index\\\\dollar.csv\", sep = \";\") time.sleep(5) employment = query_bc(25239) employment.to_csv(\"..\\\\data\\\\economic_index\\\\employment.csv\", sep = \";\") time.sleep(5) gov_debt = query_bc(4503) gov_debt.to_csv(\"..\\\\data\\\\economic_index\\\\gov_debt.csv\", sep = \";\") time.sleep(5) consumer_confidence = query_bc(4393) consumer_confidence.to_csv(\"..\\\\data\\\\economic_index\\\\consumer_confidence.csv\", sep = \";\") time.sleep(5) Vamos gerar um dataframe final incluindo todas as s\u00e9ries temporais econ\u00f4micas, note que existir\u00e3o c\u00e9lulas vazias pois os indicadores econ\u00f4micos apresentam frequ\u00eancias diferentes, podem ser di\u00e1rios, mensais, trimestrais ou anuais, e tamb\u00e9m os indicadores come\u00e7aram a ser utilizados em momentos distintos na hist\u00f3ria. economic_time_series = pd.merge(ipca, igpm, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(inpc, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(selic_meta, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(international_reserve, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(pnad, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(cdi, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(gdp, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(dollar, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(employment, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(gov_debt, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(consumer_confidence, left_index = True, right_index= True, how= 'outer') economic_time_series.columns = [\"ipca\", \"igpm\", \"inpc\", \"selic_meta\", \"international_reserve\", \"pnad\", \"cdi\", \"gdp\", \"dollar\", \"employment\", \"gov_debt\", \"consumer_confidence\"] economic_time_series.to_csv(\"..\\\\data\\\\economic_index\\\\economic_time_series.csv\", sep = \";\")","title":"Coleta de dados"},{"location":"data_scrapping/#raspagem-e-preparacao-dos-dados","text":"","title":"Raspagem e prepara\u00e7\u00e3o dos dados"},{"location":"data_scrapping/#dados-do-twitter","text":"A etapa inicial do projeto \u00e9 a obten\u00e7\u00e3o dos dados do Twitter, faremos o scraping (raspagem) dos dados, para isso utilizaremos de duas fontes, um conjunto de dados presente no Kraggle e um conjunto de dados obtido atrav\u00e9s da API do Twitter. import pandas as pd import tweepy as tw import pickle as pkl import getpass import time","title":"Dados do Twitter"},{"location":"data_scrapping/#dados-do-kraggle","text":"O conjunto de dados do Kraggle Jair Bolsonaro Twitter Data apresenta uma cole\u00e7\u00e3o de 3 mil tweets do Bolsonaro. kraggle_data = pd.read_csv(\"..//data//tweets//bolsonaro_tweets.csv\") kraggle_data.date = pd.to_datetime(kraggle_data.date) kraggle_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date text likes retweets link 0 2020-08-01 - Ministro @tarcisiogdf e os 160 anos da Infra... 880 163 https://twitter.com/jairbolsonaro/status/12895... 1 2020-08-01 - Coletiva com o prefeito de Bag\u00e9/RS, Divaldo ... 336 51 https://twitter.com/jairbolsonaro/status/12895... 2 2020-08-01 @rsallesmma \ud83d\udc4d\ud83c\udffb 442 39 https://twitter.com/jairbolsonaro/status/12893... 3 2020-08-01 @Marceloscf2 @tarcisiogdf Seguimos! \ud83e\udd1d 164 22 https://twitter.com/jairbolsonaro/status/12893... 4 2020-08-01 @ItamaratyGovBr @USAID \ud83d\udc4d\ud83c\udffb 223 24 https://twitter.com/jairbolsonaro/status/12893... print(\"O per\u00edodo de \" + str(len(kraggle_data))+\" tweets \u00e9 de \" + str(kraggle_data.date.min())[0:10] +\" at\u00e9 \" + str(kraggle_data.date.max())[0:10] + \".\") O per\u00edodo de 9351 tweets \u00e9 de 2010-04-01 at\u00e9 2020-08-01.","title":"Dados do Kraggle"},{"location":"data_scrapping/#twitter-api","text":"N\u00f3s tamb\u00e9m podemos obter dados sobre Jair Bolsonaro, Fl\u00e1vio Bolsonaro, Carlos Bolsonaro e Eduardo Bolsonaro da API do Twitter usando Tweepy. No entanto, o acesso \u00e9 permitido apenas para os \u00faltimos 3200 tweets, dessa forma, n\u00e3o \u00e9 o hist\u00f3rico completo de tweets. Autentica\u00e7\u00e3o consumer_key = getpass.getpass() \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 consumer_secret = getpass.getpass() \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 access_token = getpass.getpass() \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 access_token_secret = getpass.getpass() \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 auth = tw.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_token, access_token_secret) api = tw.API(auth, wait_on_rate_limit=True) #requesting data from twitter twitter_data = {} for user in ['jairbolsonaro', 'bolsonarosp', 'flaviobolsonaro', 'carlosbolsonaro']: twitter_data[user] = [tweet for tweet in tw.Cursor(api.user_timeline,id=user, tweet_mode =\"extended\").items()] #saving original data with open(\"..//data//tweets//brute_scrapping,pkl\", \"wb+\") as f: pkl.dump(twitter_data, f) #script to create a dataframe from the data date = [] text = [] user_name = [] place = [] retweets = [] favorites = [] for user in ['jairbolsonaro', 'bolsonarosp', 'flaviobolsonaro', 'carlosbolsonaro']: for tweet in twitter_data[user]: date.append(tweet._json['created_at']) text.append(tweet._json['full_text']) user_name.append(user) place.append(tweet._json['place']) retweets.append(tweet._json['retweet_count']) favorites.append(tweet._json['favorite_count']) API_data = pd.DataFrame({'name': user_name, 'text': text, 'date': pd.to_datetime(date), 'place': place, 'retweets': retweets, 'favorites':favorites}) API_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name text date place retweets favorites 0 jairbolsonaro -Edif\u00edcio Joelma/SP, 1974.\\n\\n-Sgt CASSANIGA s... 2020-07-27 20:51:13+00:00 None 3154 16202 1 jairbolsonaro - \u00c1gua para quem tem sede.\\n- Liberdade para u... 2020-07-27 11:10:36+00:00 None 8101 37357 2 jairbolsonaro @tarcisiogdf @MInfraestrutura \ud83e\udd1d\ud83c\udde7\ud83c\uddf7, Ministro! 2020-07-26 20:18:19+00:00 None 1074 16840 3 jairbolsonaro 2- @MinEconomia @MinCidadania @onyxlorenzoni @... 2020-07-26 15:40:39+00:00 None 1337 6383 4 jairbolsonaro 1- Acompanhe as redes sociais! @secomvc @fabio... 2020-07-26 15:39:47+00:00 None 3287 14836 API_data.to_csv('..API_data.csv', sep = \"~\")","title":"Twitter API"},{"location":"data_scrapping/#dados-economicos","text":"N\u00f3s vamos coletar os dados do BACEN (Banco Central do Brasil) usando sua API. N\u00f3s vamos utilizar as s\u00e9ries temporais de alguns importantes indicadores econ\u00f4micos como GDP, IPCA, NPCC. O c\u00f3digo presente neste notebook \u00e9 baseado no v\u00eddeo de C\u00f3digo Quant - Finan\u00e7as Quantitativas : COMO ACESSAR A BASE DE DADOS DO BANCO CENTRAL DO BRASIL COM PYTHON | Python para Investimentos #10 . def query_bc(code): url = 'http://api.bcb.gov.br/dados/serie/bcdata.sgs.{}/dados?formato=json'.format(code) df = pd.read_json(url) df['data'] = pd.to_datetime(df['data'], dayfirst=True) df.set_index('data', inplace=True) return df As s\u00e9ries temporais escolhidas para compor o nosso conjunto de dados est\u00e3o listadas abaixo, tabm\u00e9m est\u00e1 listado sua unidade e o instituto que publica o indicador: IPCA : \u00cdndice nacional de pre\u00e7os ao consumidor-amplo, Var. % mensal, (IBGE). Este \u00edndice mede a varia\u00e7\u00e3o de pre\u00e7os de mercado para o consumidor final e \u00e9 utilizado para medir a infla\u00e7\u00e3o. INPC: \u00cdndice nacional de pre\u00e7os ao consumidor, Var. % mensal, (IBGE). Este \u00edndice mede a varia\u00e7\u00e3o de pre\u00e7os da cesta de consumo da popula\u00e7\u00e3o assalariada com mais baixo rendimento. IGP-M : \u00cdndice geral de pre\u00e7os do mercado, Var. % mensal, (FGV). Este \u00edndice mede a varia\u00e7\u00e3o de pre\u00e7os atrav\u00e9s de outros \u00edndices, do IPCA, INCC, e IPC. Meta SELIC : Taxa b\u00e1sica de juros definida pelo Copom, % a.a., (Copom). CDI : Taxa de juros, % a.d., (Cetip). Taxa de juros para empr\u00e9stimo entre bancos. PIB : Produto Interno Bruto mensal, R\\$ (milh\u00f5es), (BCB-Depec). D\u00f3lar: Taxa de c\u00e2mbio - D\u00f3lar americano (venda), u.m.c/US\\$, (Sisbacen PTAX800). D\u00edvida p\u00fablica: D\u00edvida l\u00edquida do Setor P\u00fablico (%PIB) - Total, %, (BSB - DSTAT). Reserva Internacional: Reserva internacional total di\u00e1ria, US\\$ (milh\u00f5es), (BCB - DSTAT). \u00cdndice de emprego formal, (Mtb). PNADC: Taxa de desocupa\u00e7\u00e3o, %, (IBGE). \u00cdndice de confian\u00e7a do consumidor, (Fecomercio). ipca = query_bc(433) ipca.to_csv(\"..\\\\data\\\\economic_index\\\\ipca.csv\", sep = \";\") time.sleep(5) igpm = query_bc(189) igpm.to_csv(\"..\\\\data\\\\economic_index\\\\igpm.csv\", sep = \";\") time.sleep(5) inpc = query_bc(188) inpc.to_csv(\"..\\\\data\\\\economic_index\\\\inpc.csv\", sep = \";\") time.sleep(5) selic_meta = query_bc(432) selic_meta.to_csv(\"..\\\\data\\\\economic_index\\\\selic_meta.csv\", sep = \";\") time.sleep(5) international_reserve = query_bc(13621) international_reserve.to_csv(\"..\\\\data\\\\economic_index\\\\international_reserve.csv\", sep = \";\") time.sleep(5) pnad = query_bc(24369) pnad.to_csv(\"..\\\\data\\\\economic_index\\\\pnad.csv\", sep = \";\") time.sleep(5) cdi = query_bc(12) cdi.to_csv(\"..\\\\data\\\\economic_index\\\\cdi.csv\", sep = \";\") time.sleep(5) gdp = query_bc(4385) gdp.to_csv(\"..\\\\data\\\\economic_index\\\\gdp.csv\", sep = \";\") time.sleep(5) dollar = query_bc(1) dollar.to_csv(\"..\\\\data\\\\economic_index\\\\dollar.csv\", sep = \";\") time.sleep(5) employment = query_bc(25239) employment.to_csv(\"..\\\\data\\\\economic_index\\\\employment.csv\", sep = \";\") time.sleep(5) gov_debt = query_bc(4503) gov_debt.to_csv(\"..\\\\data\\\\economic_index\\\\gov_debt.csv\", sep = \";\") time.sleep(5) consumer_confidence = query_bc(4393) consumer_confidence.to_csv(\"..\\\\data\\\\economic_index\\\\consumer_confidence.csv\", sep = \";\") time.sleep(5) Vamos gerar um dataframe final incluindo todas as s\u00e9ries temporais econ\u00f4micas, note que existir\u00e3o c\u00e9lulas vazias pois os indicadores econ\u00f4micos apresentam frequ\u00eancias diferentes, podem ser di\u00e1rios, mensais, trimestrais ou anuais, e tamb\u00e9m os indicadores come\u00e7aram a ser utilizados em momentos distintos na hist\u00f3ria. economic_time_series = pd.merge(ipca, igpm, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(inpc, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(selic_meta, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(international_reserve, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(pnad, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(cdi, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(gdp, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(dollar, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(employment, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(gov_debt, left_index = True, right_index= True, how= 'outer') economic_time_series = economic_time_series.merge(consumer_confidence, left_index = True, right_index= True, how= 'outer') economic_time_series.columns = [\"ipca\", \"igpm\", \"inpc\", \"selic_meta\", \"international_reserve\", \"pnad\", \"cdi\", \"gdp\", \"dollar\", \"employment\", \"gov_debt\", \"consumer_confidence\"] economic_time_series.to_csv(\"..\\\\data\\\\economic_index\\\\economic_time_series.csv\", sep = \";\")","title":"Dados econ\u00f4micos"},{"location":"preprocessing/","text":"Pr\u00e9-processamento dos dados Nessa etapa, iremos realizar o pr\u00e9-processamento dos dados para apresenta-los em um resultado adequado para a an\u00e1lise explorat\u00f3ria dos dados e em sequ\u00eancia para o desenvolvimento de modelos. \u00c9 necess\u00e1rio selecionar as informa\u00e7\u00f5es \u00fateis obtidas dos tweets e organaliza-las em um dataframe do Pandas . import pandas as pd import pickle as pkl import tweepy as tw Preparado os dados obtidos atrav\u00e9s da API do Twitter As informa\u00e7\u00f5es selecionadas para compor o conjunto de dados processado ser\u00e3o as seguintes (e os nomes presente no dataframe ): Nome do usus\u00e1rio que postou o tweet (name). Data de cria\u00e7\u00e3o do tweet (created_at). Texto completo do tweet (full_text). Hashtags presentes no tweet (hashtags). Men\u00e7\u00f5es a usu\u00e1rios presentes no tweet (user_mentions). Tipo de m\u00eddia presente no tweet, se h\u00e1 m\u00eddia (media_type). Se o tweet \u00e9 um reply (status_reply). N\u00famero de retweets que o tweet recebeu (retweet_count). N\u00fam\u00e9ro de likes que o tweet recebeu (favorite_count). #opening API resulted data with open('..//data//tweets//brute_scrapping.pkl', 'rb') as f: tweets_data = pkl.load(f) #create dataframe from tweets data #going to use the following information from tweets: #### created_at : date of post #### full_text: tweet text #### hashtags: all hashtags in the tweet #### user_mentions: all mentions in the tweet #### media_type: type of the media in the tweet, if there is media #### status_reply: if the tweet is a reply to a status #### name: username @ #### retweet_count: number of retweets #### favorite_count: number of retweets tweets_dict = {} columns_list = ['created_at', 'full_text', 'hashtags', 'user_mentions', 'media_type', 'status_reply','name', 'retweet_count', 'favorite_count'] for var in columns_list: tweets_dict[var] = [] for user in ['jairbolsonaro', 'bolsonarosp', 'flaviobolsonaro', 'carlosbolsonaro']: for tweet in tweets_data[user]: for var in columns_list: if var == 'hashtags': aux = '/'.join([u['text'] for u in tweet._json['entities'][var]]) if aux == \"\": tweets_dict[var].append(\"None\") else: tweets_dict[var].append(aux) elif var == \"user_mentions\": aux = '/'.join([u['screen_name'] for u in tweet._json['entities'][var]]) if aux== \"\": tweets_dict[var].append(\"None\") else: tweets_dict[var].append(aux) elif var == \"status_reply\": tweets_dict[var].append(1 if tweet._json['in_reply_to_status_id'] != None else 0) elif var == \"name\": tweets_dict[var].append(user) elif var == \"media_type\": if 'media' in list(tweet._json['entities'].keys()): tweets_dict[var].append(tweet._json['extended_entities']['media'][0]['type']) else: tweets_dict[var].append(None) else: tweets_dict[var].append(tweet._json[var]) tweets_df = pd.DataFrame(tweets_dict) Em seguida, iremos criar vari\u00e1veis extras a partir da vari\u00e1vel da data de postagem do tweet, que ser\u00e3o as seguintes: Ano de postagem (year). M\u00eas de postagem (month). Dia de postagem (day). Hora de postagem (hour). Minuto de postagem (minute). Dia da semana de postagem (weekday). Essas vari\u00e1veis auxiliar\u00e3o para o processo de modelagem. Al\u00e9m delas, iremos criar vari\u00e1veis indicadoras, que ser\u00e3o: Se o tweet possui hashtags (has_hashtags). Se o tweet possui men\u00e7\u00f5es (has_mentions). Se o tweet possui alguma m\u00eddia (has_midia). tweets_df['date'] = pd.to_datetime(tweets_df.created_at) tweets_df['year'] = tweets_df.date.apply(lambda x : x.year) tweets_df['month'] = tweets_df.date.apply(lambda x : x.month) tweets_df['day'] = tweets_df.date.apply(lambda x : x.day) tweets_df['hour'] = tweets_df.date.apply(lambda x : x.hour) tweets_df['minute'] = tweets_df.date.apply(lambda x : x.minute) tweets_df['weekday'] = tweets_df.date.apply(lambda x : x.weekday) tweets_df['has_hashtags'] = tweets_df.hashtags.apply(lambda x : 1 if x != \"None\" else 0) tweets_df['has_mentions'] = tweets_df.user_mentions.apply(lambda x : 1 if x != \"None\" else 0) tweets_df['has_media'] = tweets_df.media_type.apply(lambda x : 1 if x != \"None\" else 0) tweets_df.drop(columns = ['created_at'], inplace = True) tweets_df.to_csv(\"..\\\\data\\\\tweets\\\\preprocessed_tweets.csv\", sep = \"~\")","title":"Pr\u00e9-processamento"},{"location":"preprocessing/#pre-processamento-dos-dados","text":"Nessa etapa, iremos realizar o pr\u00e9-processamento dos dados para apresenta-los em um resultado adequado para a an\u00e1lise explorat\u00f3ria dos dados e em sequ\u00eancia para o desenvolvimento de modelos. \u00c9 necess\u00e1rio selecionar as informa\u00e7\u00f5es \u00fateis obtidas dos tweets e organaliza-las em um dataframe do Pandas . import pandas as pd import pickle as pkl import tweepy as tw","title":"Pr\u00e9-processamento dos dados"},{"location":"preprocessing/#preparado-os-dados-obtidos-atraves-da-api-do-twitter","text":"As informa\u00e7\u00f5es selecionadas para compor o conjunto de dados processado ser\u00e3o as seguintes (e os nomes presente no dataframe ): Nome do usus\u00e1rio que postou o tweet (name). Data de cria\u00e7\u00e3o do tweet (created_at). Texto completo do tweet (full_text). Hashtags presentes no tweet (hashtags). Men\u00e7\u00f5es a usu\u00e1rios presentes no tweet (user_mentions). Tipo de m\u00eddia presente no tweet, se h\u00e1 m\u00eddia (media_type). Se o tweet \u00e9 um reply (status_reply). N\u00famero de retweets que o tweet recebeu (retweet_count). N\u00fam\u00e9ro de likes que o tweet recebeu (favorite_count). #opening API resulted data with open('..//data//tweets//brute_scrapping.pkl', 'rb') as f: tweets_data = pkl.load(f) #create dataframe from tweets data #going to use the following information from tweets: #### created_at : date of post #### full_text: tweet text #### hashtags: all hashtags in the tweet #### user_mentions: all mentions in the tweet #### media_type: type of the media in the tweet, if there is media #### status_reply: if the tweet is a reply to a status #### name: username @ #### retweet_count: number of retweets #### favorite_count: number of retweets tweets_dict = {} columns_list = ['created_at', 'full_text', 'hashtags', 'user_mentions', 'media_type', 'status_reply','name', 'retweet_count', 'favorite_count'] for var in columns_list: tweets_dict[var] = [] for user in ['jairbolsonaro', 'bolsonarosp', 'flaviobolsonaro', 'carlosbolsonaro']: for tweet in tweets_data[user]: for var in columns_list: if var == 'hashtags': aux = '/'.join([u['text'] for u in tweet._json['entities'][var]]) if aux == \"\": tweets_dict[var].append(\"None\") else: tweets_dict[var].append(aux) elif var == \"user_mentions\": aux = '/'.join([u['screen_name'] for u in tweet._json['entities'][var]]) if aux== \"\": tweets_dict[var].append(\"None\") else: tweets_dict[var].append(aux) elif var == \"status_reply\": tweets_dict[var].append(1 if tweet._json['in_reply_to_status_id'] != None else 0) elif var == \"name\": tweets_dict[var].append(user) elif var == \"media_type\": if 'media' in list(tweet._json['entities'].keys()): tweets_dict[var].append(tweet._json['extended_entities']['media'][0]['type']) else: tweets_dict[var].append(None) else: tweets_dict[var].append(tweet._json[var]) tweets_df = pd.DataFrame(tweets_dict) Em seguida, iremos criar vari\u00e1veis extras a partir da vari\u00e1vel da data de postagem do tweet, que ser\u00e3o as seguintes: Ano de postagem (year). M\u00eas de postagem (month). Dia de postagem (day). Hora de postagem (hour). Minuto de postagem (minute). Dia da semana de postagem (weekday). Essas vari\u00e1veis auxiliar\u00e3o para o processo de modelagem. Al\u00e9m delas, iremos criar vari\u00e1veis indicadoras, que ser\u00e3o: Se o tweet possui hashtags (has_hashtags). Se o tweet possui men\u00e7\u00f5es (has_mentions). Se o tweet possui alguma m\u00eddia (has_midia). tweets_df['date'] = pd.to_datetime(tweets_df.created_at) tweets_df['year'] = tweets_df.date.apply(lambda x : x.year) tweets_df['month'] = tweets_df.date.apply(lambda x : x.month) tweets_df['day'] = tweets_df.date.apply(lambda x : x.day) tweets_df['hour'] = tweets_df.date.apply(lambda x : x.hour) tweets_df['minute'] = tweets_df.date.apply(lambda x : x.minute) tweets_df['weekday'] = tweets_df.date.apply(lambda x : x.weekday) tweets_df['has_hashtags'] = tweets_df.hashtags.apply(lambda x : 1 if x != \"None\" else 0) tweets_df['has_mentions'] = tweets_df.user_mentions.apply(lambda x : 1 if x != \"None\" else 0) tweets_df['has_media'] = tweets_df.media_type.apply(lambda x : 1 if x != \"None\" else 0) tweets_df.drop(columns = ['created_at'], inplace = True) tweets_df.to_csv(\"..\\\\data\\\\tweets\\\\preprocessed_tweets.csv\", sep = \"~\")","title":"Preparado os dados obtidos atrav\u00e9s da API do Twitter"}]}